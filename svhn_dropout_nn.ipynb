{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "МЕТОДЫ:\n",
        "1. Conv Net + max-pooling\n",
        "2. Conv Net + max pooling + dropout in fully connected layers\n",
        "3. Conv Net + max pooling + dropout in all layers\n",
        "4. Conv Net + maxout\n",
        "\n"
      ],
      "metadata": {
        "id": "9AnNhlVkqPmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Используется устройство: {device}')"
      ],
      "metadata": {
        "id": "GRQnZjDaqFQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95771fbe-5a15-45bc-d350-080b8d1805fd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Используется устройство: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_svhn_data():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Нормализация в [-1, 1]\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.SVHN(root='./data', split='train', download=True, transform=transform)\n",
        "    test_dataset = datasets.SVHN(root='./data', split='test', download=True, transform=transform)\n",
        "\n",
        "    indices = torch.randperm(len(train_dataset))[:10000]\n",
        "    train_dataset = torch.utils.data.Subset(train_dataset, indices)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"Обучающих образцов: {len(train_dataset)}\")\n",
        "    print(f\"Тестовых образцов: {len(test_dataset)}\")\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "PJCsorBFqoSd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNetMaxPooling(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.0):\n",
        "        super(ConvNetMaxPooling, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, padding=2)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        if self.dropout_rate > 0:\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        if self.dropout_rate > 0:\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool3(x)\n",
        "        if self.dropout_rate > 0:\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        if self.dropout_rate > 0:\n",
        "            x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WX-Udt70rITw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNetMaxPoolingDropoutFC(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(ConvNetMaxPoolingDropoutFC, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, padding=2)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "_ILuxR35rIcG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Maxout(nn.Module):\n",
        "    #Maxout: y_i = max_{j=1..k} (w_{ij} * x + b_{ij})\n",
        "    #num_features — входная размерность\n",
        "    #num_units \"выходных нейронов\"\n",
        "    #k            — количество аффинных копий (pieces), из которых берётся max. Для каждого нейрона вычисляется k линейных преобразований, и берется максимум из них.\n",
        "    def __init__(self, num_features, num_units, k=2):\n",
        "        super(Maxout, self).__init__()\n",
        "        self.num_units = num_units\n",
        "        self.k = k\n",
        "        # Единый Linear слой, выдающий num_units * k значений → затем reshape + max\n",
        "        self.linear = nn.Linear(num_features, num_units * k)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, num_features)\n",
        "        output = self.linear(x)  # → (B, num_units * k)\n",
        "        output = output.view(-1, self.num_units, self.k)  # → (B, num_units, k). Разделяем выходы на группы по k для каждого нейрона.\n",
        "        return torch.max(output, dim=2)[0]  # → (B, num_units). Выбираем максимум из k копий для каждого нейрона.\n",
        "\n",
        "class ConvNetMaxout(nn.Module):\n",
        "    def __init__(self, num_classes=10, k=2, dropout_rate=0.0): # Добавляем dropout_rate\n",
        "        super(ConvNetMaxout, self).__init__()\n",
        "        self.k = k\n",
        "        self.dropout_rate = dropout_rate # Сохраняем dropout rate\n",
        "\n",
        "        # Conv1: 3 → 32*k → maxout → 32\n",
        "        self.conv1 = nn.Conv2d(3, 32 * k, kernel_size=5, padding=2)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        if self.dropout_rate > 0:\n",
        "            self.dropout_conv1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Conv2: 32 → 64*k → maxout → 64\n",
        "        self.conv2 = nn.Conv2d(32, 64 * k, kernel_size=5, padding=2)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        if self.dropout_rate > 0:\n",
        "            self.dropout_conv2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Conv3: 64 → 128*k → maxout → 128\n",
        "        self.conv3 = nn.Conv2d(64, 128 * k, kernel_size=5, padding=2)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        if self.dropout_rate > 0:\n",
        "            self.dropout_conv3 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # После pool3: 128 x 4 x 4 (если вход 32x32)\n",
        "        self.fc1 = Maxout(num_features=128 * 4 * 4, num_units=512, k=k)\n",
        "        if self.dropout_rate > 0:\n",
        "            self.dropout_fc = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        x = self.conv1(x)  # (B, 32*k, H, W)\n",
        "        x = x.view(x.size(0), 32, self.k, x.size(2), x.size(3))  # (B, 32, k, H, W)\n",
        "        x = torch.max(x, dim=2)[0]  # (B, 32, H, W)\n",
        "        x = self.pool1(x)\n",
        "        if self.dropout_rate > 0:\n",
        "            x = self.dropout_conv1(x)\n",
        "\n",
        "        # Layer 2\n",
        "        x = self.conv2(x)  # (B, 64*k, H, W)\n",
        "        x = x.view(x.size(0), 64, self.k, x.size(2), x.size(3))\n",
        "        x = torch.max(x, dim=2)[0]\n",
        "        x = self.pool2(x)\n",
        "        if self.dropout_rate > 0:\n",
        "            x = self.dropout_conv2(x)\n",
        "\n",
        "        # Layer 3\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(x.size(0), 128, self.k, x.size(2), x.size(3))\n",
        "        x = torch.max(x, dim=2)[0]\n",
        "        x = self.pool3(x)\n",
        "        if self.dropout_rate > 0:\n",
        "            x = self.dropout_conv3(x)\n",
        "\n",
        "        # Classifier\n",
        "        x = x.view(x.size(0), -1)  # (B, 128*4*4)\n",
        "        x = self.fc1(x)  # Maxout → (B, 512)\n",
        "        if self.dropout_rate > 0:\n",
        "            x = self.dropout_fc(x)\n",
        "        x = self.fc2(x)  # (B, num_classes)\n",
        "        return x"
      ],
      "metadata": {
        "id": "CIyDX1htG77p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNetMaxPoolingDropoutAll(nn.Module):\n",
        "    #Dropout с вероятностями (0.9, 0.75, 0.75, 0.5, 0.5, 0.5)\n",
        "    def __init__(self, use_dropout=True):\n",
        "        super(ConvNetMaxPoolingDropoutAll, self).__init__()\n",
        "        self.use_dropout = use_dropout\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 96, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool1 = nn.MaxPool2d(3, stride=2, padding=1)\n",
        "        self.dropout1 = nn.Dropout(0.1)  # p=0.9 -> dropout=0.1\n",
        "\n",
        "        self.conv2 = nn.Conv2d(96, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool2 = nn.MaxPool2d(3, stride=2, padding=1)\n",
        "        self.dropout2 = nn.Dropout(0.25)  # p=0.75 -> dropout=0.25\n",
        "\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool3 = nn.MaxPool2d(3, stride=2, padding=1)\n",
        "        self.dropout3 = nn.Dropout(0.25)  # p=0.75 -> dropout=0.25\n",
        "\n",
        "        # После трех pooling размер будет примерно 4x4\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 2048)\n",
        "        self.dropout4 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc2 = nn.Linear(2048, 2048)\n",
        "        self.dropout5 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc3 = nn.Linear(2048, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = x.view(-1, 256 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout4(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout5(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def apply_max_norm_constraint(self, c=4.0):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "                weight = module.weight.data\n",
        "                norm = weight.norm(2, dim=1, keepdim=True)\n",
        "                weight = weight * torch.clamp(c / norm, max=1.0)\n",
        "                module.weight.data = weight"
      ],
      "metadata": {
        "id": "mx-Qcw-Hstr9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, num_epochs=10, use_max_norm=False):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.95, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if use_max_norm:\n",
        "                model.apply_max_norm_constraint(c=4.0)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "\n",
        "    return train_losses"
      ],
      "metadata": {
        "id": "PdwV8QqSstvS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_loss = 0.0\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    avg_loss = test_loss / len(test_loader)\n",
        "    return accuracy, avg_loss"
      ],
      "metadata": {
        "id": "6XjTlFG1st9z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YU1RYibDpcBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e55e27f3-3a1e-4c6a-c0a1-64422171fa40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 182M/182M [00:04<00:00, 42.1MB/s]\n",
            "100%|██████████| 64.3M/64.3M [00:01<00:00, 32.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обучающих образцов: 10000\n",
            "Тестовых образцов: 26032\n",
            "Conv Net + max-pooling\n",
            "Epoch [2/10], Loss: 2.2366\n",
            "Epoch [4/10], Loss: 1.9664\n",
            "Epoch [6/10], Loss: 0.6675\n",
            "Epoch [8/10], Loss: 0.4467\n",
            "Epoch [10/10], Loss: 0.3139\n",
            "Точность на тесте: 82.58%, Потеря: 0.6234\n",
            "Conv Net + max-pooling + dropout в FC слоях\n",
            "Epoch [2/10], Loss: 2.2370\n",
            "Epoch [4/10], Loss: 1.9554\n",
            "Epoch [6/10], Loss: 0.7910\n",
            "Epoch [8/10], Loss: 0.5334\n",
            "Epoch [10/10], Loss: 0.3824\n",
            "Точность на тесте: 84.79%, Потеря: 0.5261\n",
            "Conv Net + MaxPool + Dropout in all layers\n",
            "Epoch [2/15], Loss: 2.2394\n",
            "Epoch [4/15], Loss: 2.2103\n",
            "Epoch [6/15], Loss: 1.8301\n",
            "Epoch [8/15], Loss: 1.1419\n",
            "Epoch [10/15], Loss: 0.7740\n",
            "Epoch [12/15], Loss: 0.5598\n",
            "Epoch [14/15], Loss: 0.4562\n",
            "Точность на тесте: 86.26%, Потеря: 0.4676\n",
            "Conv Net + maxout\n",
            "Epoch [2/10], Loss: 2.1032\n",
            "Epoch [4/10], Loss: 0.6602\n",
            "Epoch [6/10], Loss: 0.2609\n",
            "Epoch [8/10], Loss: 0.1245\n",
            "Epoch [10/10], Loss: 0.0439\n",
            "Точность на тесте: 86.42%, Потеря: 0.6795\n",
            "Conv Net + maxout + Dropout\n",
            "Epoch [2/10], Loss: 2.2347\n",
            "Epoch [4/10], Loss: 1.6316\n",
            "Epoch [6/10], Loss: 0.8435\n",
            "Epoch [8/10], Loss: 0.6748\n",
            "Epoch [10/10], Loss: 0.5901\n",
            "Точность на тесте: 86.42%, Потеря: 0.6795\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    train_loader, test_loader = load_svhn_data()\n",
        "    results = {}\n",
        "\n",
        "    print(\"Conv Net + max-pooling\")\n",
        "    model1 = ConvNetMaxPooling(dropout_rate=0.0)\n",
        "    train_losses1 = train_model(model1, train_loader, num_epochs=10)\n",
        "    acc1, loss1 = evaluate_model(model1, test_loader)\n",
        "    results['ConvNet_MaxPooling_NoDropout'] = {'accuracy': acc1, 'loss': loss1}\n",
        "    print(f\"Точность на тесте: {acc1:.2f}%, Потеря: {loss1:.4f}\")\n",
        "\n",
        "    print(\"Conv Net + max-pooling + dropout в FC слоях\")\n",
        "    model2 = ConvNetMaxPoolingDropoutFC(dropout_rate=0.5)\n",
        "    train_losses2 = train_model(model2, train_loader, num_epochs=10)\n",
        "    acc2, loss2 = evaluate_model(model2, test_loader)\n",
        "    results['ConvNet_MaxPooling_DropoutFC'] = {'accuracy': acc2, 'loss': loss2}\n",
        "    print(f\"Точность на тесте: {acc2:.2f}%, Потеря: {loss2:.4f}\")\n",
        "\n",
        "    print(\"Conv Net + MaxPool + Dropout in all layers\")\n",
        "    model10 = ConvNetMaxPoolingDropoutAll()\n",
        "    train_losses10 = train_model(model10, train_loader, num_epochs=15, use_max_norm=True)\n",
        "    acc10, loss10 = evaluate_model(model10, test_loader)\n",
        "    results['ConvNet_MaxPooling_DropoutAll'] = {'accuracy': acc10, 'loss': loss10}\n",
        "    print(f\"Точность на тесте: {acc10:.2f}%, Потеря: {loss10:.4f}\")\n",
        "\n",
        "    print(\"Conv Net + maxout\")\n",
        "    model5 = ConvNetMaxout()\n",
        "    train_losses5 = train_model(model5, train_loader, num_epochs=10)\n",
        "    acc5, loss5 = evaluate_model(model5, test_loader)\n",
        "    results['ConvNet_Maxout'] = {'accuracy': acc5, 'loss': loss5}\n",
        "    print(f\"Точность на тесте: {acc5:.2f}%, Потеря: {loss5:.4f}\")\n",
        "\n",
        "    print(\"Conv Net + maxout + Dropout\")\n",
        "    model6 = ConvNetMaxout(dropout_rate=0.5)\n",
        "    train_losses6 = train_model(model6, train_loader, num_epochs=10)\n",
        "    acc6, loss6 = evaluate_model(model5, test_loader)\n",
        "    results['ConvNet_Maxout'] = {'accuracy': acc6, 'loss': loss6}\n",
        "    print(f\"Точность на тесте: {acc6:.2f}%, Потеря: {loss6:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacked Sparse Autoencoders - многослойные разреженные автоэнкодеры\n",
        "# class SparseAutoencoder(nn.Module):\n",
        "#     #Использует KL-дивергенцию для разреженности.\n",
        "#     def __init__(self, input_dim, hidden_dim, sparsity_target=0.05, sparsity_weight=0.1):\n",
        "#         super(SparseAutoencoder, self).__init__()\n",
        "#         self.input_dim = input_dim\n",
        "#         self.hidden_dim = hidden_dim\n",
        "#         self.sparsity_target = sparsity_target\n",
        "#         self.sparsity_weight = sparsity_weight\n",
        "\n",
        "#         self.encoder = nn.Linear(input_dim, hidden_dim)\n",
        "#         self.decoder = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         encoded = F.relu(self.encoder(x))\n",
        "#         decoded = self.decoder(encoded)\n",
        "#         return encoded, decoded\n",
        "\n",
        "#     def kl_divergence(self, rho_hat):\n",
        "#         #Вычисляет KL-дивергенцию для разреженности.\n",
        "#         #rho_hat - средняя активация скрытых единиц\n",
        "#         rho = torch.full_like(rho_hat, self.sparsity_target)\n",
        "#         kl = self.sparsity_target * torch.log(self.sparsity_target / (rho_hat + 1e-8)) + \\\n",
        "#              (1 - self.sparsity_target) * torch.log((1 - self.sparsity_target) / (1 - rho_hat + 1e-8))\n",
        "#         return kl.sum()\n",
        "\n",
        "# class StackedSparseAutoencoder(nn.Module):\n",
        "#     #предобучениe признаков -> классификатор\n",
        "#     def __init__(self, input_dim=3072, hidden_dims=[512, 256], num_classes=10, use_dropout=False):\n",
        "#         super(StackedSparseAutoencoder, self).__init__()\n",
        "#         self.use_dropout = use_dropout\n",
        "\n",
        "#         layers = []\n",
        "#         dims = [input_dim] + hidden_dims\n",
        "\n",
        "#         for i in range(len(dims) - 1):\n",
        "#             layers.append(nn.Linear(dims[i], dims[i+1]))\n",
        "#             layers.append(nn.ReLU())\n",
        "#             if use_dropout:\n",
        "#                 layers.append(nn.Dropout(0.5))\n",
        "\n",
        "#         self.encoder = nn.Sequential(*layers)\n",
        "\n",
        "#         self.classifier = nn.Linear(hidden_dims[-1], num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x.view(x.size(0), -1)  # Flatten\n",
        "#         features = self.encoder(x)\n",
        "#         output = self.classifier(features)\n",
        "#         return output\n",
        "\n",
        "    #print(\"Stacked Sparse Autoencoders БЕЗ dropout\")\n",
        "    # model14 = StackedSparseAutoencoder(input_dim=32*32*3, hidden_dims=[512, 256], use_dropout=False)\n",
        "    # train_losses14 = train_model(model14, train_loader, num_epochs=10)\n",
        "    # acc14, loss14 = evaluate_model(model14, test_loader)\n",
        "    # results['StackedSparseAE_NoDropout'] = {'accuracy': acc14, 'loss': loss14}\n",
        "    # print(f\"Точность на тесте: {acc14:.2f}%, Потеря: {loss14:.4f}\")\n",
        "\n",
        "    # print(\"Stacked Sparse Autoencoders С dropout\")\n",
        "    # model15 = StackedSparseAutoencoder(input_dim=32*32*3, hidden_dims=[512, 256], use_dropout=True)\n",
        "    # train_losses15 = train_model(model15, train_loader, num_epochs=10)\n",
        "    # acc15, loss15 = evaluate_model(model15, test_loader)\n",
        "    # results['StackedSparseAE_WithDropout'] = {'accuracy': acc15, 'loss': loss15}\n",
        "    # print(f\"Точность на тесте: {acc15:.2f}%, Потеря: {loss15:.4f}\")"
      ],
      "metadata": {
        "id": "Xcs6LPLtrIN4"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}