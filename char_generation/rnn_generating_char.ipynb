{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "371ddc85-5aad-45c4-aadd-52d14b4a24db",
      "metadata": {
        "id": "371ddc85-5aad-45c4-aadd-52d14b4a24db"
      },
      "source": [
        "Char generation using rnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "280ee9ae-0c58-4a09-8371-8e57f1e08956",
      "metadata": {
        "id": "280ee9ae-0c58-4a09-8371-8e57f1e08956"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from text_helpers import load_corpus, Vocab, batch_generator, encode_text, decode_text, generate_seed, sample_from_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5e1d027a-2b29-45f6-9379-8de21e3def91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e1d027a-2b29-45f6-9379-8de21e3def91",
        "outputId": "1d44c2ae-af60-4c72-fcb1-da841dc039ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "77166ec0-0e15-40c5-8418-2646b1e68871",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77166ec0-0e15-40c5-8418-2646b1e68871",
        "outputId": "5575e788-e69b-4289-cd3c-b8f39dc65cb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<unk>': 0, ' ': 1, 'e': 2, 't': 3, 'o': 4, 'a': 5, 'h': 6, 's': 7, 'r': 8, 'n': 9, 'i': 10, '\\n': 11, 'l': 12, 'd': 13, 'u': 14, 'm': 15, 'y': 16, ',': 17, 'w': 18, 'f': 19, 'c': 20, 'g': 21, 'I': 22, 'b': 23, 'p': 24, ':': 25, '.': 26, 'A': 27, 'v': 28, 'k': 29, 'T': 30, \"'\": 31, 'E': 32, 'O': 33, 'N': 34, 'R': 35, 'S': 36, 'L': 37, 'C': 38, ';': 39, 'W': 40, 'U': 41, 'H': 42, 'M': 43, 'B': 44, '?': 45, 'G': 46, '!': 47, 'D': 48, '-': 49, 'F': 50, 'Y': 51, 'P': 52, 'K': 53, 'V': 54, 'j': 55, 'q': 56, 'x': 57, 'z': 58, 'J': 59, 'Q': 60, 'Z': 61, 'X': 62, '3': 63, '&': 64, '$': 65}\n",
            "CharRNN(\n",
            "  (embedding): Embedding(66, 32)\n",
            "  (rnn): RNN(32, 128, num_layers=2)\n",
            "  (linear): Linear(in_features=128, out_features=66, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 2/69632 [00:00<1:23:07, 13.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of batches: 272.\n",
            "effective text length: 1114112.\n",
            "x shape:  (64, 17408)\n",
            "y shape:  (64, 17408)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 256, Loss: 656.33: 100%|██████████| 69632/69632 [1:14:33<00:00, 15.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 1 7 2]\n",
            "tensor([[5],\n",
            "        [1],\n",
            "        [7],\n",
            "        [2]])\n",
            "torch.Size([4, 1, 66])\n",
            "Sampled index: 1\n",
            "\n",
            "Generating text after training:\n",
            "KING RICHARD: hinde h ar t at s areronore tharendeno at terese what wit se t s wnd,\n",
            "\n",
            "INIUSAREONGLAne s anond wind this allor shenonder athie areshan to t athite herer tho athes t t thangheroutherd,'s alinou tore ther sere tore wicathouth at then t alllle thino an t and tharouse serer wed al t sher t tou thour theste tharenge hindere thin he s at se t s we t wond wing s shithand tonou al ar ander ande atouse sthes the s the herit angerono sther thint wore t te arond sthat t astour matho whis se whes whon the\n"
          ]
        }
      ],
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, batch_first=False) # RNN expects (seq_len, batch_size, input_size)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size) # to project RNN output to vocab size\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(0.01)\n",
        "            elif isinstance(m, nn.RNN):\n",
        "                for name, param in m.named_parameters():\n",
        "                    if 'weight' in name:\n",
        "                        nn.init.orthogonal_(param)\n",
        "                    elif 'bias' in name:\n",
        "                        nn.init.constant_(param, 0)\n",
        "\n",
        "    def forward(self, input, hidden=None):\n",
        "        # input shape: (seq_len, batch_size)\n",
        "        embedded = self.embedding(input) # shape (seq_len, batch_size, embedding_size)\n",
        "\n",
        "        if hidden is None:\n",
        "            hidden = torch.zeros(self.num_layers, input.size(1), self.hidden_size).to(input.device) # input.size(1) = batch_size\n",
        "\n",
        "        output, hidden = self.rnn(embedded, hidden) # output: (seq_len, batch_size, hidden_size), hidden: (num_layers, batch_size, hidden_size)\n",
        "        output = self.linear(output) # output: (seq_len, batch_size, vocab_size)\n",
        "        return output, hidden\n",
        "\n",
        "def build_rnn(vocab_size, embedding_size, hidden_size, num_layers):\n",
        "    return CharRNN(vocab_size, embedding_size, hidden_size, num_layers)\n",
        "\n",
        "def train(net, train_loader, device, num_epochs, learning_rate, num_batches):\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    loss_function = torch.nn.CrossEntropyLoss()\n",
        "    loss_history = []\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    with tqdm(total=num_batches*num_epochs, position=0, leave=True) as pbar:\n",
        "        for epoch in range(num_epochs):\n",
        "            running_loss = 0.0\n",
        "            hidden = None # Initialize hidden state for each epoch\n",
        "\n",
        "            for _ in range(num_batches):\n",
        "                inputs, labels, *_ = next(train_loader)\n",
        "                inputs = torch.from_numpy(inputs).to(device) # shape: [seq_len, batch_size]\n",
        "                labels = torch.from_numpy(labels).to(device) # shape: [seq_len, batch_size]\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                output, hidden = net(inputs, hidden)\n",
        "                hidden = hidden.detach() # Detach hidden state for next batch\n",
        "\n",
        "                loss = loss_function(output.reshape(-1, VOCAB_SIZE), labels.reshape(-1))\n",
        "                loss.backward()\n",
        "                # Clip gradients to prevent exploding gradients (common in RNNs)\n",
        "                torch.nn.utils.clip_grad_norm_(net.parameters(), 5)\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "                pbar.update(1) # Update for each batch\n",
        "\n",
        "            pbar.set_description(\"Epoch: %d, Loss: %.2f\" % (epoch + 1, running_loss))\n",
        "            loss_history.append(running_loss)\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "# read text\n",
        "corpus, vocab = load_corpus('/content/tinyshakespeare.txt', token_type = 'char')\n",
        "VOCAB_SIZE = len(vocab)\n",
        "\n",
        "print(vocab._token_to_idx) # print vocab index\n",
        "\n",
        "EMMBEDDING_SIZE = 32\n",
        "HIDDEN_SIZE = 128\n",
        "NUM_LAYERS = 2\n",
        "\n",
        "model = build_rnn(VOCAB_SIZE, EMMBEDDING_SIZE, HIDDEN_SIZE, NUM_LAYERS).to(device)\n",
        "print(model)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "SEQ_LENGTH = 64\n",
        "EPOCHS = 256\n",
        "LR = 0.001\n",
        "\n",
        "num_batches = (len(corpus) - 1) // (BATCH_SIZE * SEQ_LENGTH)\n",
        "data_iter = batch_generator(encode_text(corpus, char2id=vocab), batch_size = BATCH_SIZE, seq_len=SEQ_LENGTH, vocab = vocab)\n",
        "\n",
        "train(model, data_iter, device, EPOCHS, LR, num_batches)\n",
        "\n",
        "seed = generate_seed(corpus)\n",
        "encoded = encode_text(seed, char2id=vocab) # encode in 1D numpy array\n",
        "\n",
        "print(encoded)\n",
        "\n",
        "tensor = torch.tensor(encoded, dtype=torch.long).unsqueeze(1).to(device) #unsqueeze to make it (seq_len, 1)\n",
        "print(tensor)\n",
        "\n",
        "# Pass initial hidden state for inference example\n",
        "output, hidden = model(tensor, hidden=None)\n",
        "print(output.shape)\n",
        "\n",
        "# output[-1, 0, :] selects the logits for the last character of the sequence, for the first (and only) batch item.\n",
        "# This gives a 1D tensor of shape [vocab_size].\n",
        "probs = torch.softmax(output[-1, 0, :], dim=0)\n",
        "\n",
        "sampled_index = sample_from_probs(probs.detach().cpu().numpy(), top_n=4)\n",
        "print(f\"Sampled index: {sampled_index}\")\n",
        "\n",
        "def generate_text(model, seed_chars, length=256, top_n=4, vocab=None, device=None):\n",
        "    model.eval()\n",
        "    generated_chars_list = list(seed_chars)\n",
        "\n",
        "    encoded_seed = encode_text(seed_chars, char2id=vocab) #numpy array\n",
        "    input_tensor = torch.tensor(encoded_seed, dtype=torch.long).unsqueeze(1).to(device)\n",
        "    hidden = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if input_tensor.numel() > 0: # If seed is not empty\n",
        "            output, hidden = model(input_tensor, hidden)\n",
        "            last_logits = output[-1, 0, :] # shape: (vocab_size)\n",
        "        else:\n",
        "            random_idx = torch.randint(0, len(vocab._idx_to_token), (1,)).item()\n",
        "            input_tensor = torch.tensor([[random_idx]], dtype=torch.long).to(device)\n",
        "            output, hidden = model(input_tensor, hidden) # Process this single char to get initial hidden state and logits\n",
        "            last_logits = output[0, 0, :]\n",
        "            generated_chars_list.append(vocab._idx_to_token[random_idx])\n",
        "\n",
        "    probs = torch.softmax(last_logits, dim=0)\n",
        "    current_input_idx = sample_from_probs(probs.detach().cpu().numpy(), top_n=top_n)\n",
        "    generated_chars_list.append(vocab._idx_to_token[current_input_idx])\n",
        "\n",
        "    while len(generated_chars_list) < length:\n",
        "        input_tensor_one_char = torch.tensor([[current_input_idx]], dtype=torch.long).to(device) # (1, 1)\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model(input_tensor_one_char, hidden)\n",
        "            # Output for a single character input will be (1, 1, vocab_size)\n",
        "            logits = output[0, 0, :]\n",
        "\n",
        "            probs = torch.softmax(logits, dim=0)\n",
        "            current_input_idx = sample_from_probs(probs.detach().cpu().numpy(), top_n=top_n)\n",
        "            generated_chars_list.append(vocab._idx_to_token[current_input_idx])\n",
        "\n",
        "    model.train()\n",
        "    return ''.join(generated_chars_list)\n",
        "\n",
        "print(\"\\nGenerating text after training:\")\n",
        "generated_text_output = generate_text(model, \"KING RICHARD: \", length=512, top_n=4, vocab=vocab, device=device)\n",
        "print(generated_text_output)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}